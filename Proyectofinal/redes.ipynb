{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab0c1ed",
   "metadata": {},
   "source": [
    "# PROYECTO REDES NEURONALES CON TENSORFLOW \n",
    "\n",
    "Este proyecto busca dar una idea del 'cómo se hace' y 'por qué de los resultados' de un modelo simple de clasifiación de red neuronal convolucional (CNN)  \n",
    "\n",
    "## Requerimientos\n",
    "\n",
    "Este proyecto es hecho en python, requiere ciertas librerías para que funcione correctamente, es por eso  que se requiere su instalación previa, lo primero es que se está usando una versión de python 3.11 (usa ```python --version``` para verificar tu versión de python en el entorno virtual), y se usan las siguientes librerías: \n",
    " \n",
    "\n",
    "<details>\n",
    "  <summary>Tensorflow</summary>\n",
    "\n",
    "```shell\n",
    "pip install tensorflow tensorflow-datasets\n",
    "pip install tensorflow[and-cuda] #si se tiene tarjeta de video para uso de core CUDA\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>OS</summary>\n",
    "\n",
    "Ya viene con python, no requiere instalación. Si requiere instalación use: \n",
    "```shell\n",
    "pip install os\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>pandas</summary>\n",
    "\n",
    "```shell\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Scikit-learn | sklearn</summary>\n",
    "\n",
    "```shell\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859241cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166c211",
   "metadata": {},
   "source": [
    "## Configuración \n",
    "Aquí importan donde está guardados los assets del proyecto, también es importante tener en cuenta donde se encuentra el entorno virtual ese es tu raiz. \n",
    "<!-- TOC -->\n",
    "- [IMG_DIR] es ruta del directorio donde se encuentran las imagenes\n",
    "- [ATTR_FILE] es la ruta de los atributos normalizados y organizados, documento list_attr_celeba.csv \n",
    "- [IMG_SIZE] Es el tamaño de la imagen a trabajar, el tamaño de las imagenes es de 178x218 y se ajustan a 128x128\n",
    "- [BATCH_SIZE] es el tamaño de los lotes de datos, grupitos de 32 imagenes \n",
    "- [SELECTED_ATTRS] se seleccionan 3 atributos de los normalizados en el documento list_attr_celeba.csv, ``` 'Smiling', 'Male', 'Young' ```, de los posibles 40 atributos que tiene el documento\n",
    "<!-- /TOC -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"ProyectoFinal/img_align_celeba\"\n",
    "ATTR_FILE = \"ProyectoFinal/list_attr_celeba.csv\"\n",
    "IMG_SIZE = (128, 128) \n",
    "BATCH_SIZE = 32 \n",
    "SELECTED_ATTRS = ['Smiling', 'Male', 'Young']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543dd475",
   "metadata": {},
   "source": [
    "## Carga y preparación de los datos \n",
    " \n",
    " Uso de la libería pandas para definir el dataframe seleccionando las columnas de interés, es importante notar que el set de list_attr_celeba.csv usa binario de manera (-1, 1), y nostros vamos a usar (0, 1) donde 0 es -1, se hace reemplazo también aquí\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b575c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ATTR_FILE)\n",
    "df.set_index('image_id', inplace=True)\n",
    "df = df[SELECTED_ATTRS]\n",
    "df = df.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b4fcf",
   "metadata": {},
   "source": [
    "## División del data set\n",
    "\n",
    "Ahora se tiene hacer la división de los datos, para el ejemplo se usa un 80% para el entrenamiento y un 20% para las pruebas; random_state=42 es para que se pueda repetir el experimento, divide siempre igual, si se desea que cada vez sea diferente se elimina esta parte (seed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ac881",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9d32e",
   "metadata": {},
   "source": [
    "## Carga de las imagenes \n",
    "\n",
    "Función de carga de imagenes, carga una imagen desde el disco y la convierte en un array (valores 0 y 1), también devuelve la etiqueta correspondiente \n",
    "``` image_id_str = image_id.numpy().decode('utf-8')   ``` convierte de tensor que es un array de bytes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26312cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_labels(image_id, label):\n",
    "    image_id_str = image_id.numpy().decode('utf-8')  \n",
    "    img_path = os.path.join(IMG_DIR, image_id_str)\n",
    "    img = load_img(img_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return img_array, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c98fd",
   "metadata": {},
   "source": [
    "## Conversión tf.data.Dataset \n",
    "\n",
    "Convierte el dataframe en un objeto tipo ``` tf.data.Dataset ```, este es un tipo de objeto que usa tensorflow y representa una secuencia de elementos ( ejemplos de entrenamiento ) Cada elemento puede ser un para (x,y) (dato, etiqueta) o cualquier estructura de datos. \n",
    "\n",
    "Sirve para crear piplines eficientes de entrada de datos ( lectura, transformación, batching ...) es crucial para entrenar modelos grandes.\n",
    "\n",
    "### ¿Por qué se usa tf.data.Dataset? \n",
    "<!-- TOC -->\n",
    "\n",
    "-[Esalable] Maneja grandes volúmenes de datos que no caben en memoria\n",
    "-[Eficiente] Usa pipelines paralelos, caching, prefetching (técnicas de optimización en ``` tf.data.Dataset ```)\n",
    "-[Flexible] se puede hacer transformaiciones complejas y ser leidos de archivos\n",
    "-[Integración_directa_con-model.fit()] los modelos Keras aceptan Dataset como entrada\n",
    "\n",
    "<!-- /TOC -->\n",
    "\n",
    "Usa ``` tf.py_function ``` para integrar funciones de python puras dentro del pipline de TensorFlow\n",
    "\n",
    "Al final de la función se hace la conversión del los dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe):\n",
    "    image_ids = dataframe.index.values\n",
    "    labels = dataframe.values.astype('float32')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_ids, labels))\n",
    "\n",
    "    def process(x, y):\n",
    "        img, label = tf.py_function(load_image_and_labels, [x, y], [tf.float32, tf.float32])\n",
    "        img.set_shape((128, 128, 3))  # Establecer forma explícita\n",
    "        label.set_shape((len(SELECTED_ATTRS),))\n",
    "        return img, label\n",
    "\n",
    "    dataset = dataset.map(process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_ds = df_to_dataset(train_df)\n",
    "val_ds = df_to_dataset(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7ed6e",
   "metadata": {},
   "source": [
    "## Definición del modelo\n",
    "\n",
    "Aquí se define una red neuronal convolucional (CNN) con 3 capas convolucionales y una capa densa final con activación sigmoid para clasificación multietiquieta\n",
    "\n",
    "### Redes neuronales convolucionales (CNN)\n",
    "\n",
    "Se usan sobre todo para análisis de imágenes\n",
    "\n",
    "- Las capas convolucionales (``` Conv2D ```) detectan patronas visuales (bordes, texturas ...)\n",
    "- Las capas de pooling ( ``` MaxPooling2D ```) reducen el tamaño y conservan lo importante\n",
    "- Las capas densas ( ``` Dense ```) conectan todos los nodos y toman decisiones (clasificación)\n",
    "\n",
    "\n",
    "### ¿Qué es keras? \n",
    "\n",
    "Keras es una API de alto nivel para construir y entrenar redes neuronales, desde la versión 2.x de tensorflow keras está integrada como tf.keras\n",
    "\n",
    "ventajas \n",
    "- Fácil de usar y leer\n",
    "- Permite construir modelos rápidamente ( Sequential, Functional, Subclasssing ) \n",
    "- Se integra directamente con el entrenamiento, evaluación y exportación de modelos\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "la función sigmoid  $$σ(x) = \\frac{1}{1 + e^{-x}}$$, donde x es el valor de entrada ( puede venir de una neurona o de una capa densa )\n",
    "\n",
    "- toma cualquier número real y lo aplana a un valor entre 0 y 1\n",
    "- Se interpreta como una probabilidad \n",
    "\n",
    "En la clasificación multietiqueta cada ejemplo puede tener más de una clase verdadera al mismo tiempo ( para nuestro caso puede ser un hombre joven sonriendo ``` 'Smiling', 'Male', 'Young' ``` ) \n",
    "- cada neurona de salida actúa de forma independiente, indicando la probabilidad de que esa etiqueta esté presente\n",
    "\n",
    "si hay 5 etiquetas ``` [ 0.02, 0.97, 0.22, 0.01, 0.86 ] ``` se puede interpretar como que las etiquetas 2 y 5 están presentes (verdadera)\n",
    "\n",
    "### Softmax \n",
    "\n",
    " - [sigmoid] se usa par multietiqueta, cada salida es independiente \n",
    " - [softmax] se usa para multiclase, donde solo una etiqueta es verdadera y las probabilidades suman 1 \n",
    "\n",
    "\n",
    " ``` tf.keras.layers.Dense(len(SELECTED_ATTRS), activation='sigmoid') ``` Una neurona por etiqueta y devuelve un valor entre 0 y 1 (sigmoid) indicando la probabilidad de que esa etiqeuta esté presente en la imagen \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(SELECTED_ATTRS), activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e1edb",
   "metadata": {},
   "source": [
    "## Compilación y entrenamiento\n",
    "\n",
    "- Con el optimizador Adam y la función de pérdida binary_crossentropy se hace la compilación \n",
    "- se entrena durante  5 épocas \n",
    "\n",
    "``` model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) ``` método para preparar el modelo apra el entrenamiento, se requiere definir: \n",
    "- Un optimizador ( ``adam`` )\n",
    "- Una función de pérdida ( ``binary_crossentropy`` )\n",
    "- Mérticas para seguimiento ( ``accuracy`` )\n",
    "\n",
    "### Optimizador Adam \n",
    "\n",
    "El optimizador ajusta los pesos del modelo en cada paso del entrenamiento para que la función pédida disminuya, el llamado `` motor de aprendizaje ``\n",
    "Adam (  Adaptive Moment Estimation ) es un algoritmo de optimización avanzado, combina lo mejor de otros algoritmos: \n",
    "- Momentum usa el promedio de gradientes pasados\n",
    "- RMSProp adapta la tasa de aprendizaje a cada parámetro \n",
    "\n",
    "de los optimizadores mas usados por eficacia y facilidad de uso \n",
    "\n",
    "ventajas \n",
    "- Rápido y preciso \n",
    "- Pcoas cosas que se deben ajustaar ( se puede usar con valores por defecto )\n",
    "- Ideal para grandes datasets y redes profundas \n",
    "\n",
    "### Función de pérdida binary_crossentropy \n",
    "\n",
    "Se usa para clasificaciones multietiqueta o binarios, En multietiqueta cálcula una pérdidad por cada etiqueta y luego se hace un promedio \n",
    "\n",
    "\n",
    "### Entrenamiento \n",
    "\n",
    "El entrenamiento es el proceso donde el modelo: \n",
    "\n",
    "1. Toma un lote de datos (batch) \n",
    "2. Hace predicciones \n",
    "3. Calcula el error usando la función de pérdida ( ``binary_crossentropy`` )\n",
    "4. Ajusta los pesos usando el optimizador (`` Adam `` )\n",
    "\n",
    "\n",
    "- Se imprime la pérdida ( ` loss ` ) y la exactitud ( `accuracy ` ) del entrenamiento y validación\n",
    "- Si la pérdida baja, el modelo está aprendiendo \n",
    "- Puede detener el entrenamiento cuando estés satisfecho o usando técnicas como el ` early stopping `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
